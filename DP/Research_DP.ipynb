{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridworldMDP Class\n",
    "\n",
    "This class represents the gridworld environment. It initializes with a given grid shape (default is a 4x4 grid) and creates a dictionary P to store transition probabilities and rewards for each state-action pair.\n",
    "\n",
    "- __init__ : Initializes the MDP with the specified grid shape. It generates transition probabilities and rewards for each state-action pair and stores them in the P dictionary.\n",
    "- render : Renders the gridworld by printing it to the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridworldMDP:\n",
    "    \n",
    "    def __init__(self, shape=[4,4]):\n",
    "        if not isinstance(shape, (list,tuple)) or not len(shape) == 2:\n",
    "            raise ValueError(\"shape argument must be a list/tuple of length 2\")\n",
    "        \n",
    "        self.shape = shape \n",
    "        self.num_states = np.prod(shape) # 16 for 4x4 grid\n",
    "        self.num_actions = 4  # UP, RIGHT, DOWN, LEFT\n",
    "        MAX_Y = shape[0]\n",
    "        MAX_X = shape[1]\n",
    "\n",
    "        P = {}  # Transition probabilities\n",
    "        grid = np.arange(self.num_states).reshape(shape) # 4x4 Matrix grid 0 to 15 values\n",
    "        print(grid)\n",
    "        it = np.nditer(grid, flags=['multi_index']) # np.nditer is a powerful tool in NumPy for iterating over elements of arrays. It allows you to iterate over multidimensional arrays in a very flexible manner, providing control over the order of iteration and the data access pattern.\n",
    "        while not it.finished:\n",
    "            s = it.iterindex\n",
    "            y, x = it.multi_index\n",
    "\n",
    "            P[s] = {a: [] for a in range(self.num_actions)} # representing matrix in form {0: {0: [], 1: [], 2: [], 3: []}, 1: {0: [], 1: [], 2: [], 3: []}, 2:....\n",
    "            \n",
    "            is_terminal = lambda s: s == 0 or s == (self.num_states - 1) # store if the state is terminal state or not\n",
    "            reward = 0.0 if is_terminal(s) else -1.0 # if the current state is terminal then reward is 0 else it is -1\n",
    "\n",
    "            if is_terminal(s):  # if it is a terminal state, then the transition probabilities for all actions leading from this state will result in staying in the same state with a reward associated with the terminal state. This is because there are no valid actions that move away from the terminal state.\n",
    "                P[s][UP] = [(1.0,s, reward, True)] # each action's transition probability tuple (p, s', r, is_terminal) will have a probability of 1.0 for staying in the same state s, the same reward associated with the terminal state, and is_terminal set to True.\n",
    "                P[s][RIGHT] = [(1.0, s, reward, True)]\n",
    "                P[s][DOWN] = [(1.0, s, reward, True)]\n",
    "                P[s][LEFT] = [(1.0, s, reward, True)]\n",
    "            else:\n",
    "                ns_up = s if y == 0 else s - MAX_X # If the agent is already at the top row (i.e., y == 0), meaning it cannot move further up, ns_up remains the same as the current state s. Otherwise, if the agent is not at the top row, ns_up is calculated by subtracting the width of the grid (MAX_X) from the current state s, effectively moving the agent one row up.\n",
    "                ns_right = s if x == (MAX_X - 1) else s + 1 # If the agent is already at the rightmost column (i.e., x == (MAX_X - 1)), meaning it cannot move further to the right, ns_right remains the same as the current state s. Otherwise, if the agent is not at the rightmost column, ns_right is calculated by adding 1 to the current state s, effectively moving the agent one column to the right.\n",
    "                ns_down = s if y == (MAX_Y - 1) else s + MAX_X # If the agent is already at the bottom row (i.e., y == (MAX_Y - 1)), meaning it cannot move further down, ns_down remains the same as the current state s. Otherwise, if the agent is not at the bottom row, ns_down is calculated by adding the width of the grid (MAX_X) to the current state s, effectively moving the agent one row down.\n",
    "                ns_left = s if x == 0 else s - 1 # If the agent is already at the leftmost column (i.e., x == 0), meaning it cannot move further to the left, ns_left remains the same as the current state s. Otherwise, if the agent is not at the leftmost column, ns_left is calculated by subtracting 1 from the current state s, effectively moving the agent one column to the left.\n",
    "                P[s][UP] = [(1.0, ns_up, reward, is_terminal(ns_up))] # For the UP action from state s, the transition probability tuple (1.0, ns_up, reward, is_terminal(ns_up)) is assigned to P[s][UP]\n",
    "                P[s][RIGHT] = [(1.0, ns_right, reward, is_terminal(ns_right))] # Similarly, for the RIGHT action from state s, the transition probability tuple (1.0, ns_right, reward, is_terminal(ns_right)) is assigned to P[s][RIGHT].\n",
    "                P[s][DOWN] = [(1.0, ns_down, reward, is_terminal(ns_down))] #  the DOWN action from state s, the transition probability tuple (1.0, ns_down, reward, is_terminal(ns_down)) is assigned to P[s][DOWN]\n",
    "                P[s][LEFT] = [(1.0, ns_left, reward, is_terminal(ns_left))] # LEFT action from state s, the transition probability tuple (1.0, ns_left, reward, is_terminal(ns_left)) is assigned to P[s][LEFT]\n",
    "                            \n",
    "            it.iternext()\n",
    "        print(P)\n",
    "        self.P = P\n",
    "    \n",
    "    def render(self):\n",
    "        '''render/print the gridworld to the terminal'''\n",
    "        grid = np.arange(self.num_states).reshape(self.shape)\n",
    "        it = np.nditer(grid, flags=['multi_index']) # It then iterates through each element of the grid using NumPy's nditer() function.\n",
    "        while not it.finished: \n",
    "            s = it.iterindex  #  it retrieves the current state s and its corresponding row y and column x indices.\n",
    "            y, x = it.multi_index \n",
    "\n",
    "            if s == 0 or s == self.num_states - 1: # Based on the current state s, it determines whether it represents a terminal state (i.e., the starting state or the goal state). If so, it sets output to \" T \", indicating a terminal state; otherwise, it sets output to \" o \"\n",
    "                output = \" T \"\n",
    "            else:\n",
    "                output = \" o \"\n",
    "\n",
    "            if x == 0: \n",
    "                output = output.lstrip() # If the current column x is the leftmost column, it removes leading whitespace from the output\n",
    "            if x == self.shape[1]-1:\n",
    "                output = output.rstrip()\n",
    "            \n",
    "            sys.stdout.write(output)\n",
    "\n",
    "            if x == self.shape[1] - 1:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "            \n",
    "            it.iternext()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmdp = GridworldMDP()\n",
    "gmdp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value(x, mdp, form='.2f'):\n",
    "    \"\"\" Print a value function array in a nice format.\"\"\"\n",
    "    x = x.reshape(mdp.shape)\n",
    "    print('\\n'.join(' '.join(' ' + str(format(cell, form)) if cell >= 0 else str(format(cell, '.2f')) for cell in row) for row in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_deterministic_policy(policy, mdp):\n",
    "    \"\"\" Print a policy array in a nice format.\"\"\"\n",
    "    action_dict = {0: 'U', 1: 'R', 2: 'D', 3: 'L'}\n",
    "    policy = np.array([action_dict[x] for x in np.argmax(policy, axis=1)]).reshape(mdp.shape)\n",
    "    policy[0, 0] = '-'\n",
    "    policy[-1, -1] = '-'\n",
    "    print('\\n'.join(' '.join(str(cell) for cell in row) for row in policy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_value(mdp):\n",
    "    \"\"\" Returns a initialized value function array for given MDP.\"\"\"\n",
    "    return np.zeros(mdp.num_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(mdp):\n",
    "    \"\"\" Returns the random policy for a given MDP.\n",
    "    policy[x][y] is the probability of action with y for state x.\n",
    "    \"\"\"\n",
    "    return np.ones([mdp.num_states, mdp.num_actions]) / mdp.num_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_one_step(mdp, V, policy, discount=0.99):\n",
    "    V_new = V.copy()\n",
    "    for s in range(mdp.num_states):\n",
    "        for a, action_prob in enumerate(policy[s]):\n",
    "            for prob, next_state, reward, done in mdp.P[s][a]:\n",
    "                V+= action_prob * prob * (reward + discount * V[next_state])\n",
    "        \n",
    "        V_new[s] = V\n",
    "        \n",
    "    return V_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(mdp, policy, discount=0.99, theta=0.00001):\n",
    "    V = init_value(mdp)\n",
    "    max_delta = theta + 1\n",
    "    \n",
    "    while max_delta > theta:\n",
    "        V_new = policy_evaluation_one_step(mdp, V, policy, discount)\n",
    "        max_delta = np.max(np.abs(V-V_new))\n",
    "        V = V_new\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
